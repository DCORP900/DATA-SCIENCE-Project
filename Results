Accuracy: 0.9996: Out of all predictions, 99.96% were correct. However, in imbalanced datasets (like fraud detection), accuracy can be misleading â€” a model that predicts everything as "not fraud" will still have high accuracy.
Precision: 0.9747: When the model predicted "fraud", it was correct 97.47% of the time. High precision means very few false alarms (false positives).
Recall: 0.7857: Out of all actual fraud cases, the model detected 78.57%. This shows how well it catches real frauds. A lower recall means some frauds were missed (false negatives).
F1-Score: 0.8701: A balance between precision and recall. 87.01% is strong and shows the model handles both catching fraud and avoiding false alarms well.
Matthews Correlation Coefficient (MCC): 0.8749: A more balanced score (from -1 to +1) even when classes are imbalanced. A value of 0.8749 is very good, it means the model is making strong, balanced predictions overall.
